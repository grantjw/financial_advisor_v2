{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6650aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from structured_rag import (\n",
    "    get_database,\n",
    "    get_service_context,\n",
    "    get_query_engine,\n",
    "    get_response,\n",
    "    delete_database, \n",
    "    get_llm\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "from llama_index.callbacks import CallbackManager, WandbCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b81678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "121f7356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import (\n",
    "    SimpleDirectoryReader,\n",
    "    load_index_from_storage,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    ServiceContext\n",
    ")\n",
    "# from llama_index.vector_stores.faiss import FaissVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "91802054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12891a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ed36f3a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meklavyaj\u001b[0m (\u001b[33maccenture-capstone-23\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/usr/bin/nvidia-modprobe: unrecognized option: \"-s\"\n",
      "\n",
      "ERROR: Invalid commandline, please run `/usr/bin/nvidia-modprobe --help`\n",
      "       for usage information.\n",
      "\n",
      "/usr/bin/nvidia-modprobe: unrecognized option: \"-s\"\n",
      "\n",
      "ERROR: Invalid commandline, please run `/usr/bin/nvidia-modprobe --help`\n",
      "       for usage information.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/moto/edu/engi4800/ej2487/Code/Notebooks/wandb/run-20231116_032215-iqe6fr19</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/accenture-capstone-23/llama-index/runs/iqe6fr19' target=\"_blank\">royal-moon-4</a></strong> to <a href='https://wandb.ai/accenture-capstone-23/llama-index' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/accenture-capstone-23/llama-index' target=\"_blank\">https://wandb.ai/accenture-capstone-23/llama-index</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/accenture-capstone-23/llama-index/runs/iqe6fr19' target=\"_blank\">https://wandb.ai/accenture-capstone-23/llama-index/runs/iqe6fr19</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/accenture-capstone-23/llama-index/runs/iqe6fr19?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2aade82c3310>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='llama-index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c1054a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_args = {\"project\": \"llama-index\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2288773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_callback = WandbCallbackHandler(run_args=wandb_args)\n",
    "# llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5f319be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([wandb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c79c8789",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"/moto/edu/engi4800/ej2487/.cache\"\n",
    "TOKEN = \"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\"\n",
    "MISTRAL_7B_INSTRUCT = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "DB_URL = \"/moto/edu/engi4800/ej2487/Code/Data/StructuredDB/tables.db\"\n",
    "\n",
    "EXPERIMENT_LOGGER = \"/moto/edu/engi4800/ej2487/Code/Data/Query Results/structured_rag.csv\"\n",
    "EXCEL_FILE_PATH = '/moto/edu/engi4800/ej2487/Code/Data/Portfolio Assets/portfolios.xlsx'\n",
    "SOURCE_DOCUMENTS_PATH = '/moto/edu/engi4800/ej2487/Code/Data/Scrapes/'\n",
    "ASSET_MAPPING_PATH = '/moto/edu/engi4800/ej2487/Code/Data/Portfolio Assets/asset_mapping.csv'\n",
    "\n",
    "PORTFOLIOS = [\n",
    "    \"low risk\",\n",
    "    \"moderate risk\",\n",
    "    \"medium risk\",\n",
    "    \"high risk\",\n",
    "]\n",
    "\n",
    "portfolio = PORTFOLIOS[0]\n",
    "# delete_database(DB_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096b9bad",
   "metadata": {},
   "source": [
    "## Structured RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6fd060c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa443edabbd47e1987607ed5a3388d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = get_llm(MISTRAL_7B_INSTRUCT, token=TOKEN, cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8825358b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/nvidia-modprobe: unrecognized option: \"-s\"\n",
      "\n",
      "ERROR: Invalid commandline, please run `/usr/bin/nvidia-modprobe --help`\n",
      "       for usage information.\n",
      "\n",
      "/usr/bin/nvidia-modprobe: unrecognized option: \"-s\"\n",
      "\n",
      "ERROR: Invalid commandline, please run `/usr/bin/nvidia-modprobe --help`\n",
      "       for usage information.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98387c496ab7430e8cb10d8ff97edc76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sql_database = get_database(DB_URL,\n",
    "                            EXCEL_FILE_PATH=EXCEL_FILE_PATH,\n",
    "                            portfolio=portfolio, \n",
    "                            SOURCE_DOCUMENTS_PATH=SOURCE_DOCUMENTS_PATH, \n",
    "                            csv_path=ASSET_MAPPING_PATH)\n",
    "\n",
    "service_context_mistral = get_service_context(\n",
    "    MISTRAL_7B_INSTRUCT, callback_manager=callback_manager, token=TOKEN, cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "sql_query_engine = get_query_engine(\n",
    "    sql_database=sql_database, service_context=service_context_mistral\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a50babf",
   "metadata": {},
   "source": [
    "## Unstructured RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4db1cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(input_dir=\"../Data/Scrapes/\",\n",
    "                               required_exts=['.txt'], \n",
    "                               recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49d9564a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('..',\n",
       " 'Data',\n",
       " 'Scrapes',\n",
       " 'AAPL',\n",
       " 'news',\n",
       " '15_Best_Cheap_Wireless_Earbuds_Under_50_2023-11-07.txt')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.input_files[0].parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddb73bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = reader.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2015f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, doc in zip(reader.input_files, docs):\n",
    "    ticker = file.parts[3]\n",
    "    title = file.parts[-1]\n",
    "    doc.metadata['title'] = title\n",
    "    doc.metadata['ticker'] = ticker\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b9b0f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1692"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_docs = [doc for doc in docs if \"Our engineers are working quickly to resolve the issue\" not in doc.text]\n",
    "len(final_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b209c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(final_docs, \n",
    "                                        service_context=service_context_mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42171e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unstructured_query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f00487b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = unstructured_query_engine.query(\"What is Apple's latest news?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7589315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nApple has just launched a slew of new products in a Monday night event the company dubbed \"Scary Fast.\" The event focused on the reveal of Apple\\'s new Mac lineup, simultaneously showcasing the company\\'s next-generation computer chips. The company has also warned that revenue in the holiday quarter will be about the same as last year, signaling that investors wonâ€™t see the growth rebound they were banking on.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c581dd",
   "metadata": {},
   "source": [
    "## Text2SQL + Vector Retrieval Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "081bcdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine import (\n",
    "    SQLAutoVectorQueryEngine,\n",
    "    RetrieverQueryEngine,\n",
    ")\n",
    "from llama_index.tools.query_engine import QueryEngineTool, ToolMetadata\n",
    "from llama_index.indices.vector_store import VectorIndexAutoRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd178f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.vector_store.retrievers import (\n",
    "    VectorIndexAutoRetriever,\n",
    ")\n",
    "from llama_index.vector_stores.types import MetadataInfo, VectorStoreInfo\n",
    "from llama_index.query_engine.retriever_query_engine import (\n",
    "    RetrieverQueryEngine,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "493d3014",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_info = VectorStoreInfo(\n",
    "    content_info=\"news articles about different companies\",\n",
    "    metadata_info=[\n",
    "         MetadataInfo(\n",
    "            name=\"title\", type=\"str\", description=\"The title of the news article\"\n",
    "        ), \n",
    "        MetadataInfo(\n",
    "            name=\"ticker\", type=\"str\", description=\"The ticker of the company being talked about in the article\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "vector_auto_retriever = VectorIndexAutoRetriever(\n",
    "    index, vector_store_info=vector_store_info\n",
    ")\n",
    "\n",
    "retriever_query_engine = RetrieverQueryEngine.from_args(\n",
    "    vector_auto_retriever, service_context=service_context_mistral\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "36513ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_tool = QueryEngineTool(\n",
    "    query_engine=sql_query_engine,\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"Text to SQL\",\n",
    "        description=(\n",
    "            \"Useful for translating a natural language query into a SQL query over\"\n",
    "            \"tables containing historical stock prices, mapping between tickers and names, and \"\n",
    "            \"the user portfolio.\"\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool(\n",
    "    query_engine=retriever_query_engine,\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"Vector Retrieval\",\n",
    "        description=(\n",
    "            \"Useful for answering semantic questions about different companies and their recent news\"\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a2b7dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_query_engine = SQLAutoVectorQueryEngine(\n",
    "    sql_tool, \n",
    "    vector_tool, \n",
    "    service_context=service_context_mistral,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "225195d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to log trace tree to W&B: list index out of range\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert output to JSON: '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [76]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mauto_query_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGive recent news about AAPL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/llama_index/indices/query/base.py:32\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     31\u001b[0m     str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/llama_index/query_engine/sql_join_query_engine.py:312\u001b[0m, in \u001b[0;36mSQLJoinQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# TODO: see if this can be consolidated with logic in RouterQueryEngine\u001b[39;00m\n\u001b[1;32m    311\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sql_query_tool\u001b[38;5;241m.\u001b[39mmetadata, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_other_query_tool\u001b[38;5;241m.\u001b[39mmetadata]\n\u001b[0;32m--> 312\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# pick sql query\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mind \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/llama_index/selectors/types.py:81\u001b[0m, in \u001b[0;36mBaseSelector.select\u001b[0;34m(self, choices, query)\u001b[0m\n\u001b[1;32m     79\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [_wrap_choice(choice) \u001b[38;5;28;01mfor\u001b[39;00m choice \u001b[38;5;129;01min\u001b[39;00m choices]\n\u001b[1;32m     80\u001b[0m query_bundle \u001b[38;5;241m=\u001b[39m _wrap_query(query)\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchoices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/llama_index/selectors/llm_selectors.py:110\u001b[0m, in \u001b[0;36mLLMSingleSelector._select\u001b[0;34m(self, choices, query)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# parse output\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt\u001b[38;5;241m.\u001b[39moutput_parser \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m parse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _structured_output_to_selector_result(parse)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/llama_index/output_parsers/selection.py:95\u001b[0m, in \u001b[0;36mSelectionOutputParser.parse\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m     92\u001b[0m     json_obj \u001b[38;5;241m=\u001b[39m [json_obj]\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_obj:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to convert output to JSON: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m json_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_output(json_obj)\n\u001b[1;32m     98\u001b[0m answers \u001b[38;5;241m=\u001b[39m [Answer\u001b[38;5;241m.\u001b[39mfrom_dict(json_dict) \u001b[38;5;28;01mfor\u001b[39;00m json_dict \u001b[38;5;129;01min\u001b[39;00m json_output]\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert output to JSON: '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
     ]
    }
   ],
   "source": [
    "auto_query_engine.query(\"Give recent news about AAPL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "14969989",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert output to JSON: '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43mauto_query_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the number of shares bought for Google in the user portfolio?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/llama_index/indices/query/base.py:32\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     31\u001b[0m     str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/llama_index/query_engine/sql_join_query_engine.py:312\u001b[0m, in \u001b[0;36mSQLJoinQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# TODO: see if this can be consolidated with logic in RouterQueryEngine\u001b[39;00m\n\u001b[1;32m    311\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sql_query_tool\u001b[38;5;241m.\u001b[39mmetadata, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_other_query_tool\u001b[38;5;241m.\u001b[39mmetadata]\n\u001b[0;32m--> 312\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# pick sql query\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mind \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/llama_index/selectors/types.py:81\u001b[0m, in \u001b[0;36mBaseSelector.select\u001b[0;34m(self, choices, query)\u001b[0m\n\u001b[1;32m     79\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [_wrap_choice(choice) \u001b[38;5;28;01mfor\u001b[39;00m choice \u001b[38;5;129;01min\u001b[39;00m choices]\n\u001b[1;32m     80\u001b[0m query_bundle \u001b[38;5;241m=\u001b[39m _wrap_query(query)\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchoices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/llama_index/selectors/llm_selectors.py:110\u001b[0m, in \u001b[0;36mLLMSingleSelector._select\u001b[0;34m(self, choices, query)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# parse output\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt\u001b[38;5;241m.\u001b[39moutput_parser \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m parse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _structured_output_to_selector_result(parse)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/llama_index/output_parsers/selection.py:95\u001b[0m, in \u001b[0;36mSelectionOutputParser.parse\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m     92\u001b[0m     json_obj \u001b[38;5;241m=\u001b[39m [json_obj]\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_obj:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to convert output to JSON: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m json_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_output(json_obj)\n\u001b[1;32m     98\u001b[0m answers \u001b[38;5;241m=\u001b[39m [Answer\u001b[38;5;241m.\u001b[39mfrom_dict(json_dict) \u001b[38;5;28;01mfor\u001b[39;00m json_dict \u001b[38;5;129;01min\u001b[39;00m json_output]\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert output to JSON: '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
     ]
    }
   ],
   "source": [
    "resp = auto_query_engine.query(\"What is the number of shares bought for Google in the user portfolio?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f95e945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.selectors.llm_selectors import LLMSingleSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "08cb4bb3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/llama_index/llms/utils.py:20\u001b[0m, in \u001b[0;36mresolve_llm\u001b[0;34m(llm)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m     llm \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/llama_index/llms/openai.py:92\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, model, temperature, max_tokens, additional_kwargs, max_retries, api_key, api_type, api_base, api_version, callback_manager, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m additional_kwargs \u001b[38;5;241m=\u001b[39m additional_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m---> 92\u001b[0m api_key, api_type, api_base, api_version \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_openai_credentials\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    100\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    101\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    111\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/llama_index/llms/openai_utils.py:297\u001b[0m, in \u001b[0;36mresolve_openai_credentials\u001b[0;34m(api_key, api_type, api_base, api_version)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m api_key:\n\u001b[0;32m--> 297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(MISSING_API_KEY_ERROR_MESSAGE)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m api_key, api_type, api_base, api_version\n",
      "\u001b[0;31mValueError\u001b[0m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [69]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m router_query_engine \u001b[38;5;241m=\u001b[39m \u001b[43mRouterQueryEngine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mselector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLLMSingleSelector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_defaults\u001b[49m\u001b[43m(\u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context_mistral\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_engine_tools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43msql_tool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_tool\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/llama_index/query_engine/router_query_engine.py:108\u001b[0m, in \u001b[0;36mRouterQueryEngine.__init__\u001b[0;34m(self, selector, query_engine_tools, service_context, summarizer)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    103\u001b[0m     selector: BaseSelector,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m     summarizer: Optional[TreeSummarize] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    107\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice_context \u001b[38;5;241m=\u001b[39m service_context \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mServiceContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_defaults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector \u001b[38;5;241m=\u001b[39m selector\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_engines \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mquery_engine \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m query_engine_tools]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/llama_index/indices/service_context.py:154\u001b[0m, in \u001b[0;36mServiceContext.from_defaults\u001b[0;34m(cls, llm_predictor, llm, prompt_helper, embed_model, node_parser, llama_logger, callback_manager, system_prompt, query_wrapper_prompt, chunk_size, chunk_overlap, context_window, num_output, chunk_size_limit)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot specify both llm and llm_predictor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    153\u001b[0m     llm \u001b[38;5;241m=\u001b[39m resolve_llm(llm)\n\u001b[0;32m--> 154\u001b[0m llm_predictor \u001b[38;5;241m=\u001b[39m llm_predictor \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mLLMPredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm_predictor, LLMPredictor):\n\u001b[1;32m    156\u001b[0m     llm_predictor\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m callback_manager\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/llama_index/llm_predictor/base.py:89\u001b[0m, in \u001b[0;36mLLMPredictor.__init__\u001b[0;34m(self, llm, callback_manager, system_prompt, query_wrapper_prompt)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     83\u001b[0m     llm: Optional[LLMType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m     query_wrapper_prompt: Optional[BasePromptTemplate] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     87\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;124;03m\"\"\"Initialize params.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback_manager:\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m callback_manager\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/llama_index/llms/utils.py:22\u001b[0m, in \u001b[0;36mresolve_llm\u001b[0;34m(llm)\u001b[0m\n\u001b[1;32m     20\u001b[0m         llm \u001b[38;5;241m=\u001b[39m OpenAI()\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 22\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     23\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m******\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load OpenAI model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you intended to use OpenAI, please check your OPENAI_API_KEY.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTo disable the LLM entirely, set llm=None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m         )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     33\u001b[0m     splits \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: ******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******"
     ]
    }
   ],
   "source": [
    "router_query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(service_context=service_context_mistral),\n",
    "    query_engine_tools=[sql_tool, vector_tool],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebfad22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb39b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query_engine, query):\n",
    "    \n",
    "    time, resp, sql = get_response(\n",
    "        query_engine=query_engine, query_str=query, print_=False\n",
    "    )\n",
    "    return time, resp, sql\n",
    "\n",
    "\n",
    "input_queries = []\n",
    "times = []\n",
    "responses = []\n",
    "sql_queries = []\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        option = int(\n",
    "            input(\n",
    "                \"\"\"\n",
    "        Menu:\n",
    "            1. Enter Query\n",
    "            2. Describe Database\n",
    "            3. Exit\n",
    "\n",
    "        Enter Option (1 / 2 / 3): \n",
    "        \"\"\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    except ValueError:\n",
    "        print(\n",
    "            \"\"\"\n",
    "        Invalid Option, try again.\n",
    "        \"\"\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    print(\n",
    "        f\"\"\"\n",
    "        Option Chosen: {option}\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    if option == 1:\n",
    "        query = input(\n",
    "            \"\"\"\n",
    "        ---------------------------------\n",
    "        Enter Query:\n",
    "        \n",
    "            Sample Queries:\n",
    "                - What is the number of shares bought for Google in the user portfolio?\n",
    "                - What is the company name for the ticker 'LULU'? \n",
    "                You may have to join with the asset_mapping table on ticker to get relevant information.\n",
    "                - What asset types are contained within the user portfolio?\n",
    "        \n",
    "        ...\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "        input_queries.append(query)\n",
    "        # print(\n",
    "        #     f\"\"\"\n",
    "        # User Input:\n",
    "        # {query}\n",
    "        # \"\"\"\n",
    "        # )\n",
    "\n",
    "        print(\n",
    "            f\"\"\"\n",
    "        Getting Response...\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "        time, resp, sql = answer_query(query_engine=query_engine, query=query)\n",
    "\n",
    "        times.append(time)\n",
    "        responses.append(resp)\n",
    "        sql_queries.append(sql)\n",
    "        \n",
    "        print(\n",
    "            f\"\"\"\n",
    "        Time Taken:\n",
    "        {time:.3f} seconds\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"\"\"\n",
    "        LLM Response:\n",
    "        {resp}\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"\"\"\n",
    "        SQL Query:\n",
    "        {sql}\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "        printline()\n",
    "\n",
    "    elif option == 2:\n",
    "        tables = sql_database.get_usable_table_names()\n",
    "        print(tables)\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            df = pd.read_csv(EXPERIMENT_LOGGER)\n",
    "            new_df = pd.DataFrame({\n",
    "                'user_input':input_queries,\n",
    "                'time_taken':times,\n",
    "                'llm_response':responses,\n",
    "                'sql_query':sql_queries, \n",
    "                'model_id': MISTRAL_7B_INSTRUCT,  \n",
    "                'portfolio': portfolio,  \n",
    "            })\n",
    "            \n",
    "            df = pd.concat([df, new_df], axis=0, ignore_index=True)\n",
    "            df.to_csv(EXPERIMENT_LOGGER, index=False)\n",
    "        \n",
    "        except:\n",
    "            new_df = pd.DataFrame({\n",
    "                'user_input':input_queries,\n",
    "                'time_taken':times,\n",
    "                'llm_response':responses,\n",
    "                'sql_query':sql_queries,\n",
    "                'model_id': MISTRAL_7B_INSTRUCT,\n",
    "                'portfolio': portfolio,  \n",
    "            })\n",
    "            \n",
    "            new_df.to_csv(EXPERIMENT_LOGGER, index=False)\n",
    "            \n",
    "        print(\"Exiting...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a5b1554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. You can order the results by a relevant column to return the most interesting examples in the database.\n",
      "\n",
      "Never query for all the columns from a specific table, only ask for a few relevant columns given the question.\n",
      "\n",
      "Pay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Pay attention to which column is in which table. Also, qualify column names with the table name when needed. You are required to use the following format, each taking one line:\n",
      "\n",
      "Question: Question here\n",
      "SQLQuery: SQL Query to run\n",
      "SQLResult: Result of the SQLQuery\n",
      "Answer: Final answer here\n",
      "\n",
      "Only use tables listed below.\n",
      "{schema}\n",
      "\n",
      "Question: {query_str}\n",
      "SQLQuery: \n"
     ]
    }
   ],
   "source": [
    "DEFAULT_TEXT_TO_SQL_TMPL = (\n",
    "    \"Given an input question, first create a syntactically correct {dialect} \"\n",
    "    \"query to run, then look at the results of the query and return the answer. \"\n",
    "    \"You can order the results by a relevant column to return the most \"\n",
    "    \"interesting examples in the database.\\n\\n\"\n",
    "    \"Never query for all the columns from a specific table, only ask for a \"\n",
    "    \"few relevant columns given the question.\\n\\n\"\n",
    "    \"Pay attention to use only the column names that you can see in the schema \"\n",
    "    \"description. \"\n",
    "    \"Be careful to not query for columns that do not exist. \"\n",
    "    \"Pay attention to which column is in which table. \"\n",
    "    \"Also, qualify column names with the table name when needed. \"\n",
    "    \"You are required to use the following format, each taking one line:\\n\\n\"\n",
    "    \"Question: Question here\\n\"\n",
    "    \"SQLQuery: SQL Query to run\\n\"\n",
    "    \"SQLResult: Result of the SQLQuery\\n\"\n",
    "    \"Answer: Final answer here\\n\\n\"\n",
    "    \"Only use tables listed below.\\n\"\n",
    "    \"{schema}\\n\\n\"\n",
    "    \"Question: {query_str}\\n\"\n",
    "    \"SQLQuery: \"\n",
    ")\n",
    "\n",
    "print(DEFAULT_TEXT_TO_SQL_TMPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
